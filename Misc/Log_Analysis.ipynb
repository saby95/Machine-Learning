{"cells":[{"cell_type":"markdown","source":["Import SparkContext"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a8872ce-1423-4165-a17b-2108f1477b24"}}},{"cell_type":"code","source":["from pyspark import SparkContext"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c73beed3-5d04-447e-a46c-b1ae81fbb882"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Defining The solution in a routine"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68b7e968-f502-4885-b045-cb7dc052a08b"}}},{"cell_type":"code","source":["def PrintNoOfLogs(data):\n    levels = data.filter(lambda row: len(row) > 0)\\\n                 .filter(lambda row: not row.startswith(\"##\"))\\\n                 .filter(lambda row: not row.startswith(\" \"))\\\n                 .map(lambda row: (row.partition(' ')[0], 1))\n    reducedLevels = levels.reduceByKey(lambda count1, count2: count1+count2)\n    answer = reducedLevels.sortByKey().collect()\n    print(answer)  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d105cc9-9c9f-45b4-aa1a-8a3ad426720e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##Example 1\nPrint the input text file"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00be3461-612c-45f3-89ab-c7fb115801f6"}}},{"cell_type":"code","source":["dbutils.fs.cp(\"/FileStore/tables/input.txt\", \"file:///tmp/input.txt\")\nwith open(\"/tmp/input.txt\", \"r\") as file:\n    print (file.read())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b635c3a-9e95-4148-898c-11db00884e3e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"## input.txt ##\n\nINFO This is a message with content\n\nINFO This is some other content\n\n## (empty line)\n\nINFO Here are more messages\n\nWARN This is a warning\n\n \n\nERROR Something bad happened\n\nWARN More details on the bad thing\n\nINFO back to normal messages\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["## input.txt ##\n\nINFO This is a message with content\n\nINFO This is some other content\n\n## (empty line)\n\nINFO Here are more messages\n\nWARN This is a warning\n\n \n\nERROR Something bad happened\n\nWARN More details on the bad thing\n\nINFO back to normal messages\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Executing the routine on example 1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a0a14e3-604c-4d88-8c70-6117e2157bf1"}}},{"cell_type":"code","source":["sc = SparkContext.getOrCreate()\ndata = sc.textFile(\"/FileStore/tables/input.txt\")\nPrintNoOfLogs(data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0964b4d9-b5eb-41ef-8552-58784e3f22a8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[('ERROR', 1), ('INFO', 4), ('WARN', 2)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[('ERROR', 1), ('INFO', 4), ('WARN', 2)]\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##Example 2\nPrint the contents of the text file"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa7e07cc-de78-4e83-83ab-cf79ccad13da"}}},{"cell_type":"code","source":["dbutils.fs.cp(\"/FileStore/tables/input1.txt\", \"file:///tmp/input1.txt\")\nwith open(\"/tmp/input1.txt\", \"r\") as file:\n    print (file.read())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca40fc1d-42e3-4d88-9d04-fada0b885d00"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"## input1.txt ##\n\nINFO SparkContext: Running Spark version 3.2.1\nWARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nINFO ResourceUtils: ==============================================================\nINFO ResourceUtils: No custom resources configured for spark.driver.\nINFO ResourceUtils: ==============================================================\nINFO SparkContext: Submitted application: solution.py\nINFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\nINFO ResourceProfile: Limiting resource is cpu\nINFO ResourceProfileManager: Added ResourceProfile id: 0\nINFO SecurityManager: Changing view acls to: jananiravikumar\nINFO SecurityManager: Changing modify acls to: jananiravikumar\nINFO SecurityManager: Changing view acls groups to: \nINFO SecurityManager: Changing modify acls groups to: \nINFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jananiravikumar); groups with view permissions: Set(); users  with modify permissions: Set(jananiravikumar); groups with modify permissions: Set()\nINFO Utils: Successfully started service 'sparkDriver' on port 59433.\nINFO SparkEnv: Registering MapOutputTracker\nINFO SparkEnv: Registering BlockManagerMaster\nINFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\nINFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\nINFO SparkEnv: Registering BlockManagerMasterHeartbeat\nINFO DiskBlockManager: Created local directory at /private/var/folders/9h/14zx94zx7pb72dsw1xlqq1s00000gp/T/blockmgr-9c49dc93-71f8-4b11-9db4-d086b509cd82\nINFO MemoryStore: MemoryStore started with capacity 434.4 MiB\nINFO SparkEnv: Registering OutputCommitCoordinator\nINFO Utils: Successfully started service 'SparkUI' on port 4040.\nINFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.0.90:4040\nINFO Executor: Starting executor ID driver on host 10.0.0.90\nINFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59434.\nINFO NettyBlockTransferService: Server created on 10.0.0.90:59434\nINFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\nINFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.0.90, 59434, None)\nINFO BlockManagerMasterEndpoint: Registering block manager 10.0.0.90:59434 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.0.90, 59434, None)\nINFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.0.90, 59434, None)\nINFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.0.90, 59434, None)\nINFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 219.7 KiB, free 434.2 MiB)\nINFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.1 KiB, free 434.2 MiB)\nINFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.0.90:59434 (size: 32.1 KiB, free: 434.4 MiB)\nINFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\nINFO FileInputFormat: Total input files to process : 1\nINFO SparkContext: Starting job: sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7\nINFO DAGScheduler: Registering RDD 3 (reduceByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) as input to shuffle 0\nINFO DAGScheduler: Got job 0 (sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) with 2 output partitions\nINFO DAGScheduler: Final stage: ResultStage 1 (sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7)\nINFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\nINFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\nINFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7), which has no missing parents\nINFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.0 KiB, free 434.1 MiB)\nINFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 434.1 MiB)\nINFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.0.90:59434 (size: 7.2 KiB, free: 434.4 MiB)\nINFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\nINFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) (first 15 tasks are for partitions Vector(0, 1))\nINFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\nINFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.0.90, executor driver, partition 0, PROCESS_LOCAL, 4546 bytes) taskResourceAssignments Map()\nINFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (10.0.0.90, executor driver, partition 1, PROCESS_LOCAL, 4546 bytes) taskResourceAssignments Map()\nINFO Executor: Running task 0.0 in stage 0.0 (TID 0)\nINFO Executor: Running task 1.0 in stage 0.0 (TID 1)\nINFO HadoopRDD: Input split: file:/Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/input.txt:127+127\nINFO HadoopRDD: Input split: file:/Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/input.txt:0+127\nINFO PythonRunner: Times: total = 179, boot = 172, init = 7, finish = 0\nINFO PythonRunner: Times: total = 180, boot = 175, init = 5, finish = 0\nINFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1609 bytes result sent to driver\nINFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1609 bytes result sent to driver\nINFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 416 ms on 10.0.0.90 (executor driver) (1/2)\nINFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 408 ms on 10.0.0.90 (executor driver) (2/2)\nINFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \nINFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 59435\nINFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) finished in 0.466 s\nINFO DAGScheduler: looking for newly runnable stages\nINFO DAGScheduler: running: Set()\nINFO DAGScheduler: waiting: Set(ResultStage 1)\nINFO DAGScheduler: failed: Set()\nINFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7), which has no missing parents\nINFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.8 KiB, free 434.1 MiB)\nINFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 434.1 MiB)\nINFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.0.90:59434 (size: 6.2 KiB, free: 434.4 MiB)\nINFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\nINFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) (first 15 tasks are for partitions Vector(0, 1))\nINFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\nINFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (10.0.0.90, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\nINFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (10.0.0.90, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\nINFO Executor: Running task 0.0 in stage 1.0 (TID 2)\nINFO Executor: Running task 1.0 in stage 1.0 (TID 3)\nINFO ShuffleBlockFetcherIterator: Getting 2 (152.0 B) non-empty blocks including 2 (152.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Getting 2 (177.0 B) non-empty blocks including 2 (177.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\nINFO PythonRunner: Times: total = 6, boot = -193, init = 199, finish = 0\nINFO PythonRunner: Times: total = 6, boot = -193, init = 199, finish = 0\nINFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1612 bytes result sent to driver\nINFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 1612 bytes result sent to driver\nINFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 48 ms on 10.0.0.90 (executor driver) (1/2)\nINFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 46 ms on 10.0.0.90 (executor driver) (2/2)\nINFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \nINFO DAGScheduler: ResultStage 1 (sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) finished in 0.056 s\nINFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\nINFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\nINFO DAGScheduler: Job 0 finished: sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7, took 0.560612 s\nINFO SparkContext: Starting job: sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7\nINFO DAGScheduler: Got job 1 (sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) with 2 output partitions\nINFO DAGScheduler: Final stage: ResultStage 3 (sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7)\nINFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\nINFO DAGScheduler: Missing parents: List()\nINFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7), which has no missing parents\nINFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.2 KiB, free 434.1 MiB)\nINFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.1 MiB)\nINFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.0.90:59434 (size: 6.0 KiB, free: 434.3 MiB)\nINFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\nINFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (PythonRDD[7] at sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) (first 15 tasks are for partitions Vector(0, 1))\nINFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0\nINFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4) (10.0.0.90, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\nINFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 5) (10.0.0.90, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\nINFO Executor: Running task 0.0 in stage 3.0 (TID 4)\nINFO Executor: Running task 1.0 in stage 3.0 (TID 5)\nINFO ShuffleBlockFetcherIterator: Getting 2 (152.0 B) non-empty blocks including 2 (152.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO ShuffleBlockFetcherIterator: Getting 2 (177.0 B) non-empty blocks including 2 (177.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO PythonRunner: Times: total = 2, boot = -38, init = 40, finish = 0\nINFO PythonRunner: Times: total = 1, boot = -40, init = 41, finish = 0\nINFO Executor: Finished task 1.0 in stage 3.0 (TID 5). 1648 bytes result sent to driver\nINFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 5) in 13 ms on 10.0.0.90 (executor driver) (1/2)\nINFO Executor: Finished task 0.0 in stage 3.0 (TID 4). 1653 bytes result sent to driver\nINFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 14 ms on 10.0.0.90 (executor driver) (2/2)\nINFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \nINFO DAGScheduler: ResultStage 3 (sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) finished in 0.019 s\nINFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\nINFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\nINFO DAGScheduler: Job 1 finished: sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7, took 0.022479 s\nINFO SparkContext: Starting job: collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7\nINFO DAGScheduler: Registering RDD 9 (sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) as input to shuffle 1\nINFO DAGScheduler: Got job 2 (collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) with 2 output partitions\nINFO DAGScheduler: Final stage: ResultStage 6 (collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7)\nINFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\nINFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)\nINFO DAGScheduler: Submitting ShuffleMapStage 5 (PairwiseRDD[9] at sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7), which has no missing parents\nINFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 10.9 KiB, free 434.1 MiB)\nINFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 434.1 MiB)\nINFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.0.90:59434 (size: 6.7 KiB, free: 434.3 MiB)\nINFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478\nINFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (PairwiseRDD[9] at sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) (first 15 tasks are for partitions Vector(0, 1))\nINFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0\nINFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6) (10.0.0.90, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\nINFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7) (10.0.0.90, executor driver, partition 1, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\nINFO Executor: Running task 1.0 in stage 5.0 (TID 7)\nINFO Executor: Running task 0.0 in stage 5.0 (TID 6)\nINFO ShuffleBlockFetcherIterator: Getting 2 (152.0 B) non-empty blocks including 2 (152.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Getting 2 (177.0 B) non-empty blocks including 2 (177.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO PythonRunner: Times: total = 1, boot = -39, init = 40, finish = 0\nINFO Executor: Finished task 1.0 in stage 5.0 (TID 7). 1781 bytes result sent to driver\nINFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 14 ms on 10.0.0.90 (executor driver) (1/2)\nINFO PythonRunner: Times: total = 2, boot = -40, init = 42, finish = 0\nINFO Executor: Finished task 0.0 in stage 5.0 (TID 6). 1781 bytes result sent to driver\nINFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 17 ms on 10.0.0.90 (executor driver) (2/2)\nINFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \nINFO DAGScheduler: ShuffleMapStage 5 (sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) finished in 0.024 s\nINFO DAGScheduler: looking for newly runnable stages\nINFO DAGScheduler: running: Set()\nINFO DAGScheduler: waiting: Set(ResultStage 6)\nINFO DAGScheduler: failed: Set()\nINFO DAGScheduler: Submitting ResultStage 6 (PythonRDD[12] at collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7), which has no missing parents\nINFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 8.7 KiB, free 434.1 MiB)\nINFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 434.1 MiB)\nINFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.0.90:59434 (size: 5.2 KiB, free: 434.3 MiB)\nINFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1478\nINFO DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (PythonRDD[12] at collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) (first 15 tasks are for partitions Vector(0, 1))\nINFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0\nINFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8) (10.0.0.90, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\nINFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9) (10.0.0.90, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\nINFO Executor: Running task 1.0 in stage 6.0 (TID 9)\nINFO Executor: Running task 0.0 in stage 6.0 (TID 8)\nINFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO PythonRunner: Times: total = 1, boot = -17, init = 18, finish = 0\nINFO PythonRunner: Times: total = 2, boot = -18, init = 19, finish = 1\nINFO Executor: Finished task 1.0 in stage 6.0 (TID 9). 1657 bytes result sent to driver\nINFO Executor: Finished task 0.0 in stage 6.0 (TID 8). 1664 bytes result sent to driver\nINFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 11 ms on 10.0.0.90 (executor driver) (1/2)\nINFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 11 ms on 10.0.0.90 (executor driver) (2/2)\nINFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \nINFO DAGScheduler: ResultStage 6 (collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) finished in 0.017 s\nINFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\nINFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\nINFO DAGScheduler: Job 2 finished: collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7, took 0.044431 s\nINFO SparkContext: Invoking stop() from shutdown hook\nINFO SparkUI: Stopped Spark web UI at http://10.0.0.90:4040\nINFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\nINFO MemoryStore: MemoryStore cleared\nINFO BlockManager: BlockManager stopped\nINFO BlockManagerMaster: BlockManagerMaster stopped\nINFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\nINFO SparkContext: Successfully stopped SparkContext\nINFO ShutdownHookManager: Shutdown hook called\nINFO ShutdownHookManager: Deleting directory /private/var/folders/9h/14zx94zx7pb72dsw1xlqq1s00000gp/T/spark-1af241a7-e870-421e-acbe-e7bbf9b01709/pyspark-fbe9e6d1-5d3e-4464-b7c8-bd70f136f6f9\nINFO ShutdownHookManager: Deleting directory /private/var/folders/9h/14zx94zx7pb72dsw1xlqq1s00000gp/T/spark-7bf13302-05eb-4afb-9e31-e4c3b170a755\nINFO ShutdownHookManager: Deleting directory /private/var/folders/9h/14zx94zx7pb72dsw1xlqq1s00000gp/T/spark-1af241a7-e870-421e-acbe-e7bbf9b01709                                                                                                        \nINFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6) (10.0.0.90, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()                                                                                                                   \nINFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6) (10.0.0.90, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\nINFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7) (10.0.0.90, executor driver, partition 1, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\nINFO Executor: Running task 1.0 in stage 5.0 (TID 7)\nINFO Executor: Running task 0.0 in stage 5.0 (TID 6)\nINFO ShuffleBlockFetcherIterator: Getting 2 (152.0 B) non-empty blocks including 2 (152.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Getting 2 (177.0 B) non-empty blocks including 2 (177.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO PythonRunner: Times: total = 1, boot = -39, init = 40, finish = 0\nINFO Executor: Finished task 1.0 in stage 5.0 (TID 7). 1781 bytes result sent to driver\nINFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 14 ms on 10.0.0.90 (executor driver) (1/2)\nINFO PythonRunner: Times: total = 2, boot = -40, init = 42, finish = 0\nINFO Executor: Finished task 0.0 in stage 5.0 (TID 6). 1781 bytes result sent to driver\nINFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 17 ms on 10.0.0.90 (executor driver) (2/2)\nINFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \nINFO DAGScheduler: ShuffleMapStage 5 (sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) finished in 0.024 s\nINFO DAGScheduler: looking for newly runnable stages\nINFO DAGScheduler: running: Set()\nINFO DAGScheduler: waiting: Set(ResultStage 6)\nINFO DAGScheduler: failed: Set()\nINFO DAGScheduler: Submitting ResultStage 6 (PythonRDD[12] at collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7), which has no missing parents\nINFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 8.7 KiB, free 434.1 MiB)\nINFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 434.1 MiB)\nINFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.0.90:59434 (size: 5.2 KiB, free: 434.3 MiB)\nINFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1478\nINFO DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (PythonRDD[12] at collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) (first 15 tasks are for partitions Vector(0, 1))\nINFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0\nINFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8) (10.0.0.90, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\nINFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9) (10.0.0.90, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\nINFO Executor: Running task 1.0 in stage 6.0 (TID 9)\nINFO Executor: Running task 0.0 in stage 6.0 (TID 8)\nINFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO PythonRunner: Times: total = 1, boot = -17, init = 18, finish = 0\nINFO PythonRunner: Times: total = 2, boot = -18, init = 19, finish = 1\nINFO Executor: Finished task 1.0 in stage 6.0 (TID 9). 1657 bytes result sent to driver\nINFO Executor: Finished task 0.0 in stage 6.0 (TID 8). 1664 bytes result sent to driver\nINFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 11 ms on 10.0.0.90 (executor driver) (1/2)\nINFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 11 ms on 10.0.0.90 (executor driver) (2/2)\nINFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \nINFO DAGScheduler: ResultStage 6 (collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) finished in 0.017 s\nINFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\nINFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\nINFO DAGScheduler: Job 2 finished: collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7, took 0.044431 s\nINFO SparkContext: Invoking stop() from shutdown hook\nINFO SparkUI: Stopped Spark web UI at http://10.0.0.90:4040\nINFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["## input1.txt ##\n\nINFO SparkContext: Running Spark version 3.2.1\nWARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nINFO ResourceUtils: ==============================================================\nINFO ResourceUtils: No custom resources configured for spark.driver.\nINFO ResourceUtils: ==============================================================\nINFO SparkContext: Submitted application: solution.py\nINFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\nINFO ResourceProfile: Limiting resource is cpu\nINFO ResourceProfileManager: Added ResourceProfile id: 0\nINFO SecurityManager: Changing view acls to: jananiravikumar\nINFO SecurityManager: Changing modify acls to: jananiravikumar\nINFO SecurityManager: Changing view acls groups to: \nINFO SecurityManager: Changing modify acls groups to: \nINFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jananiravikumar); groups with view permissions: Set(); users  with modify permissions: Set(jananiravikumar); groups with modify permissions: Set()\nINFO Utils: Successfully started service 'sparkDriver' on port 59433.\nINFO SparkEnv: Registering MapOutputTracker\nINFO SparkEnv: Registering BlockManagerMaster\nINFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\nINFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\nINFO SparkEnv: Registering BlockManagerMasterHeartbeat\nINFO DiskBlockManager: Created local directory at /private/var/folders/9h/14zx94zx7pb72dsw1xlqq1s00000gp/T/blockmgr-9c49dc93-71f8-4b11-9db4-d086b509cd82\nINFO MemoryStore: MemoryStore started with capacity 434.4 MiB\nINFO SparkEnv: Registering OutputCommitCoordinator\nINFO Utils: Successfully started service 'SparkUI' on port 4040.\nINFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.0.90:4040\nINFO Executor: Starting executor ID driver on host 10.0.0.90\nINFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59434.\nINFO NettyBlockTransferService: Server created on 10.0.0.90:59434\nINFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\nINFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.0.90, 59434, None)\nINFO BlockManagerMasterEndpoint: Registering block manager 10.0.0.90:59434 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.0.90, 59434, None)\nINFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.0.90, 59434, None)\nINFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.0.90, 59434, None)\nINFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 219.7 KiB, free 434.2 MiB)\nINFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.1 KiB, free 434.2 MiB)\nINFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.0.90:59434 (size: 32.1 KiB, free: 434.4 MiB)\nINFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\nINFO FileInputFormat: Total input files to process : 1\nINFO SparkContext: Starting job: sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7\nINFO DAGScheduler: Registering RDD 3 (reduceByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) as input to shuffle 0\nINFO DAGScheduler: Got job 0 (sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) with 2 output partitions\nINFO DAGScheduler: Final stage: ResultStage 1 (sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7)\nINFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\nINFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\nINFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7), which has no missing parents\nINFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.0 KiB, free 434.1 MiB)\nINFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 434.1 MiB)\nINFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.0.90:59434 (size: 7.2 KiB, free: 434.4 MiB)\nINFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\nINFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) (first 15 tasks are for partitions Vector(0, 1))\nINFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\nINFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.0.90, executor driver, partition 0, PROCESS_LOCAL, 4546 bytes) taskResourceAssignments Map()\nINFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (10.0.0.90, executor driver, partition 1, PROCESS_LOCAL, 4546 bytes) taskResourceAssignments Map()\nINFO Executor: Running task 0.0 in stage 0.0 (TID 0)\nINFO Executor: Running task 1.0 in stage 0.0 (TID 1)\nINFO HadoopRDD: Input split: file:/Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/input.txt:127+127\nINFO HadoopRDD: Input split: file:/Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/input.txt:0+127\nINFO PythonRunner: Times: total = 179, boot = 172, init = 7, finish = 0\nINFO PythonRunner: Times: total = 180, boot = 175, init = 5, finish = 0\nINFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1609 bytes result sent to driver\nINFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1609 bytes result sent to driver\nINFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 416 ms on 10.0.0.90 (executor driver) (1/2)\nINFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 408 ms on 10.0.0.90 (executor driver) (2/2)\nINFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \nINFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 59435\nINFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) finished in 0.466 s\nINFO DAGScheduler: looking for newly runnable stages\nINFO DAGScheduler: running: Set()\nINFO DAGScheduler: waiting: Set(ResultStage 1)\nINFO DAGScheduler: failed: Set()\nINFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7), which has no missing parents\nINFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.8 KiB, free 434.1 MiB)\nINFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 434.1 MiB)\nINFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.0.90:59434 (size: 6.2 KiB, free: 434.4 MiB)\nINFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\nINFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) (first 15 tasks are for partitions Vector(0, 1))\nINFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\nINFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (10.0.0.90, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\nINFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (10.0.0.90, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\nINFO Executor: Running task 0.0 in stage 1.0 (TID 2)\nINFO Executor: Running task 1.0 in stage 1.0 (TID 3)\nINFO ShuffleBlockFetcherIterator: Getting 2 (152.0 B) non-empty blocks including 2 (152.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Getting 2 (177.0 B) non-empty blocks including 2 (177.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\nINFO PythonRunner: Times: total = 6, boot = -193, init = 199, finish = 0\nINFO PythonRunner: Times: total = 6, boot = -193, init = 199, finish = 0\nINFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1612 bytes result sent to driver\nINFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 1612 bytes result sent to driver\nINFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 48 ms on 10.0.0.90 (executor driver) (1/2)\nINFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 46 ms on 10.0.0.90 (executor driver) (2/2)\nINFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \nINFO DAGScheduler: ResultStage 1 (sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) finished in 0.056 s\nINFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\nINFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\nINFO DAGScheduler: Job 0 finished: sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7, took 0.560612 s\nINFO SparkContext: Starting job: sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7\nINFO DAGScheduler: Got job 1 (sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) with 2 output partitions\nINFO DAGScheduler: Final stage: ResultStage 3 (sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7)\nINFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\nINFO DAGScheduler: Missing parents: List()\nINFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7), which has no missing parents\nINFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.2 KiB, free 434.1 MiB)\nINFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.1 MiB)\nINFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.0.90:59434 (size: 6.0 KiB, free: 434.3 MiB)\nINFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\nINFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (PythonRDD[7] at sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) (first 15 tasks are for partitions Vector(0, 1))\nINFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0\nINFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4) (10.0.0.90, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\nINFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 5) (10.0.0.90, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\nINFO Executor: Running task 0.0 in stage 3.0 (TID 4)\nINFO Executor: Running task 1.0 in stage 3.0 (TID 5)\nINFO ShuffleBlockFetcherIterator: Getting 2 (152.0 B) non-empty blocks including 2 (152.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO ShuffleBlockFetcherIterator: Getting 2 (177.0 B) non-empty blocks including 2 (177.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO PythonRunner: Times: total = 2, boot = -38, init = 40, finish = 0\nINFO PythonRunner: Times: total = 1, boot = -40, init = 41, finish = 0\nINFO Executor: Finished task 1.0 in stage 3.0 (TID 5). 1648 bytes result sent to driver\nINFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 5) in 13 ms on 10.0.0.90 (executor driver) (1/2)\nINFO Executor: Finished task 0.0 in stage 3.0 (TID 4). 1653 bytes result sent to driver\nINFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 14 ms on 10.0.0.90 (executor driver) (2/2)\nINFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \nINFO DAGScheduler: ResultStage 3 (sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) finished in 0.019 s\nINFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\nINFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\nINFO DAGScheduler: Job 1 finished: sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7, took 0.022479 s\nINFO SparkContext: Starting job: collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7\nINFO DAGScheduler: Registering RDD 9 (sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) as input to shuffle 1\nINFO DAGScheduler: Got job 2 (collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) with 2 output partitions\nINFO DAGScheduler: Final stage: ResultStage 6 (collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7)\nINFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\nINFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)\nINFO DAGScheduler: Submitting ShuffleMapStage 5 (PairwiseRDD[9] at sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7), which has no missing parents\nINFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 10.9 KiB, free 434.1 MiB)\nINFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 434.1 MiB)\nINFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.0.90:59434 (size: 6.7 KiB, free: 434.3 MiB)\nINFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478\nINFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (PairwiseRDD[9] at sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) (first 15 tasks are for partitions Vector(0, 1))\nINFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0\nINFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6) (10.0.0.90, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\nINFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7) (10.0.0.90, executor driver, partition 1, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\nINFO Executor: Running task 1.0 in stage 5.0 (TID 7)\nINFO Executor: Running task 0.0 in stage 5.0 (TID 6)\nINFO ShuffleBlockFetcherIterator: Getting 2 (152.0 B) non-empty blocks including 2 (152.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Getting 2 (177.0 B) non-empty blocks including 2 (177.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO PythonRunner: Times: total = 1, boot = -39, init = 40, finish = 0\nINFO Executor: Finished task 1.0 in stage 5.0 (TID 7). 1781 bytes result sent to driver\nINFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 14 ms on 10.0.0.90 (executor driver) (1/2)\nINFO PythonRunner: Times: total = 2, boot = -40, init = 42, finish = 0\nINFO Executor: Finished task 0.0 in stage 5.0 (TID 6). 1781 bytes result sent to driver\nINFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 17 ms on 10.0.0.90 (executor driver) (2/2)\nINFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \nINFO DAGScheduler: ShuffleMapStage 5 (sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) finished in 0.024 s\nINFO DAGScheduler: looking for newly runnable stages\nINFO DAGScheduler: running: Set()\nINFO DAGScheduler: waiting: Set(ResultStage 6)\nINFO DAGScheduler: failed: Set()\nINFO DAGScheduler: Submitting ResultStage 6 (PythonRDD[12] at collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7), which has no missing parents\nINFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 8.7 KiB, free 434.1 MiB)\nINFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 434.1 MiB)\nINFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.0.90:59434 (size: 5.2 KiB, free: 434.3 MiB)\nINFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1478\nINFO DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (PythonRDD[12] at collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) (first 15 tasks are for partitions Vector(0, 1))\nINFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0\nINFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8) (10.0.0.90, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\nINFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9) (10.0.0.90, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\nINFO Executor: Running task 1.0 in stage 6.0 (TID 9)\nINFO Executor: Running task 0.0 in stage 6.0 (TID 8)\nINFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO PythonRunner: Times: total = 1, boot = -17, init = 18, finish = 0\nINFO PythonRunner: Times: total = 2, boot = -18, init = 19, finish = 1\nINFO Executor: Finished task 1.0 in stage 6.0 (TID 9). 1657 bytes result sent to driver\nINFO Executor: Finished task 0.0 in stage 6.0 (TID 8). 1664 bytes result sent to driver\nINFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 11 ms on 10.0.0.90 (executor driver) (1/2)\nINFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 11 ms on 10.0.0.90 (executor driver) (2/2)\nINFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \nINFO DAGScheduler: ResultStage 6 (collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) finished in 0.017 s\nINFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\nINFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\nINFO DAGScheduler: Job 2 finished: collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7, took 0.044431 s\nINFO SparkContext: Invoking stop() from shutdown hook\nINFO SparkUI: Stopped Spark web UI at http://10.0.0.90:4040\nINFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\nINFO MemoryStore: MemoryStore cleared\nINFO BlockManager: BlockManager stopped\nINFO BlockManagerMaster: BlockManagerMaster stopped\nINFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\nINFO SparkContext: Successfully stopped SparkContext\nINFO ShutdownHookManager: Shutdown hook called\nINFO ShutdownHookManager: Deleting directory /private/var/folders/9h/14zx94zx7pb72dsw1xlqq1s00000gp/T/spark-1af241a7-e870-421e-acbe-e7bbf9b01709/pyspark-fbe9e6d1-5d3e-4464-b7c8-bd70f136f6f9\nINFO ShutdownHookManager: Deleting directory /private/var/folders/9h/14zx94zx7pb72dsw1xlqq1s00000gp/T/spark-7bf13302-05eb-4afb-9e31-e4c3b170a755\nINFO ShutdownHookManager: Deleting directory /private/var/folders/9h/14zx94zx7pb72dsw1xlqq1s00000gp/T/spark-1af241a7-e870-421e-acbe-e7bbf9b01709                                                                                                        \nINFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6) (10.0.0.90, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()                                                                                                                   \nINFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6) (10.0.0.90, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\nINFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7) (10.0.0.90, executor driver, partition 1, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\nINFO Executor: Running task 1.0 in stage 5.0 (TID 7)\nINFO Executor: Running task 0.0 in stage 5.0 (TID 6)\nINFO ShuffleBlockFetcherIterator: Getting 2 (152.0 B) non-empty blocks including 2 (152.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Getting 2 (177.0 B) non-empty blocks including 2 (177.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO PythonRunner: Times: total = 1, boot = -39, init = 40, finish = 0\nINFO Executor: Finished task 1.0 in stage 5.0 (TID 7). 1781 bytes result sent to driver\nINFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 14 ms on 10.0.0.90 (executor driver) (1/2)\nINFO PythonRunner: Times: total = 2, boot = -40, init = 42, finish = 0\nINFO Executor: Finished task 0.0 in stage 5.0 (TID 6). 1781 bytes result sent to driver\nINFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 17 ms on 10.0.0.90 (executor driver) (2/2)\nINFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \nINFO DAGScheduler: ShuffleMapStage 5 (sortByKey at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) finished in 0.024 s\nINFO DAGScheduler: looking for newly runnable stages\nINFO DAGScheduler: running: Set()\nINFO DAGScheduler: waiting: Set(ResultStage 6)\nINFO DAGScheduler: failed: Set()\nINFO DAGScheduler: Submitting ResultStage 6 (PythonRDD[12] at collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7), which has no missing parents\nINFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 8.7 KiB, free 434.1 MiB)\nINFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 434.1 MiB)\nINFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.0.90:59434 (size: 5.2 KiB, free: 434.3 MiB)\nINFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1478\nINFO DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (PythonRDD[12] at collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) (first 15 tasks are for partitions Vector(0, 1))\nINFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0\nINFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8) (10.0.0.90, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\nINFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9) (10.0.0.90, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\nINFO Executor: Running task 1.0 in stage 6.0 (TID 9)\nINFO Executor: Running task 0.0 in stage 6.0 (TID 8)\nINFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\nINFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\nINFO PythonRunner: Times: total = 1, boot = -17, init = 18, finish = 0\nINFO PythonRunner: Times: total = 2, boot = -18, init = 19, finish = 1\nINFO Executor: Finished task 1.0 in stage 6.0 (TID 9). 1657 bytes result sent to driver\nINFO Executor: Finished task 0.0 in stage 6.0 (TID 8). 1664 bytes result sent to driver\nINFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 11 ms on 10.0.0.90 (executor driver) (1/2)\nINFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 11 ms on 10.0.0.90 (executor driver) (2/2)\nINFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \nINFO DAGScheduler: ResultStage 6 (collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7) finished in 0.017 s\nINFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\nINFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\nINFO DAGScheduler: Job 2 finished: collect at /Users/jananiravikumar/Desktop/Spring'22/DATA-228/Assignments/Assignment2/solution.py:7, took 0.044431 s\nINFO SparkContext: Invoking stop() from shutdown hook\nINFO SparkUI: Stopped Spark web UI at http://10.0.0.90:4040\nINFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Executing the routine on example 2."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7fe7a489-45cf-4097-bd99-30aec5f0ae4b"}}},{"cell_type":"code","source":["sc = SparkContext.getOrCreate()\ndata = sc.textFile(\"/FileStore/tables/input1.txt\")\nPrintNoOfLogs(data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7c52fdd-ed30-4677-8540-9f71fd47bad2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[('INFO', 247), ('WARN', 1)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[('INFO', 247), ('WARN', 1)]\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##How the routine works:\nIn the lines 2,3 and 4 of the routine, it parses each row of the text files and filters out all the empty lines whose length is 0 and also the line that start with '##' or a space. Thus all the lines that contains comments, blank line or lines with errors are ignored. Then in line 5, the `map` function maps the first word (delimited by a space ' ') to a count of 1. <br \\>\nIn line 6, all the mapped entries are reduced, accumulating the count. In line 7, the reduced keys are sorted and the key, value pairs are dispalyed in line 8."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c00be838-484b-450f-ace8-fb2e3f76ba31"}}},{"cell_type":"markdown","source":["##Space and Time Complexity\nWhen run in a cluser with a single node, the functions `filter`, `map` and `reduce` should all run in linear time since they process one line of the log at a time. Thus the lines 2-6 should run in `O(n)`, where `n` is the number of lines in the log file. In line 7, the sorting operation is performed on the reduced keys, assuming the complexity of the sorting operation `O(m log(m))`,  where m is the number of unique logging levels, this should be negligible in a large log file with a huge number of log lines in it i.e., where `n >>> m`. Therefore, when there is no parallelization, the time coplexity of the routine can be given as `O(n)`.<br/><br/> \nWhen there are more nodes available, there is a possibility to lower the time complexity of the map phase to `O(1)` since the operation on line of log is not dependant on any other line in this process. In this case, the time taken for the reduce phase can also be lowered to but still the time complexity should be linearly proportional to the number of log lines. Thus, the time complexity of the routine with maximum parallelization, can be described as `O(n)`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cda95327-ccbd-4ea6-9e47-85c57967f413"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Log_Analysis_Janani_RaviKumar","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1641266955603480}},"nbformat":4,"nbformat_minor":0}
